---
title: "Random forest with full enriched data set and no split by gender nor return time"
author: "Mikko Arvas"
date: "`r Sys.time()`"
---

```{r setup, include=FALSE}
# Import libraries that might be needed
library(tidyverse, quietly = T)
library(knitr)
library(caret)
#library(ROCR)
library(pROC)
#library(furrr)
#library(sn)
library(caTools)

library(rpart.plot)
library(rpart)
library(ranger)
library(doParallel)

source("helper_functions.R")
source("validate_stan_fit.R")
source("common.R")

#setwd("~/FRCBS/interval_prediction/src/")

save_figs <- TRUE
compute <- FALSE
input_dir <- "~/FRCBS/interval_prediction/data"
result_dir <- "~/FRCBS/Hb_predictor_container/oversampling_results"
fig_dir <- paste(result_dir, "pdf", sep="/")

set.seed(123)
# echo "rmarkdown::render('rf_caret_enriched_no_splits.Rmd', clean=TRUE,output_format='pdf_document',output_file='../results/rf_caret_enriched_no_splits_20200827b.pdf')" | R --slave
# /usr/bin/pandoc +RTS -K512m -RTS rf_caret_enriched_no_splits.utf8.md --to latex --from markdown+autolink_bare_uris+tex_math_single_backslash --output ../results/rf_caret_enriched_no_splits_20200929.tex --self-contained --highlight-style tango --latex-engine pdflatex --variable graphics --variable 'geometry:margin=1in' 
# Error: pandoc document conversion failed with error 127
# In addition: There were 33 warnings (use warnings() to see them)
# Execution halted
# Warning message:
# https://stackoverflow.com/questions/34687030/pandoc-document-conversion-failed-with-error-127
# bootstrapping seems to create some leaks
# added gc() s to see if would help but did not test yet
```

## TODO:

-test if the year has any effect ?!


```{r}


# descript <- tibble(Variable = c("donor", "Hb", "days_to_previous_fb", "age", "previous_Hb_def", 
#                                 "year", "warm_season", "consecutive_deferrals", "recent_donations",
#                                 "recent_deferrals", "hour", 
#                                 "previous_Hb", "Hb_first", "Hb_deferral","nb_donat","gender"), 
#                    Pretty = c("Donor ID", "Hemoglobin", "Days to previous full blood donation", "Age", "Previous Hb deferral", 
#                               "Year", "Warm season", "Consecutive deferrals", "Recent donations", 
#                               "Recent deferrals", "Hour", 
#                               "Previous Hb", "First Hb", "Hb deferral","Life time donations","Sex"),
#                    Type = c("Factor", "numeric", "numeric (int)", "numeric", "boolean",
#                             "numeric (int)", "boolean", "numeric (int)", "numeric (int)", "numeric (int)", "numeric",
#                             "numeric", "numeric", "boolean","numeric (int)","Factor"),
#                    Explanation = c("Donor identifier",
#                                    "Amount of Hemoglobin",
#                                    "Time (in days) between Hb measurement and previous full blood donation event",
#                                    "Age of donor",
#                                    "Indicates whether the donor was deferred from blood donation due to low hemoglobin at previous donation event",
#                                    "Year of donation",
#                                    "True if donation was given in April-September",
#                                    "Amount of times the donor has been deferred due to low hemoglobin since last succesful whole blood donation",
#                                    "Amount of donations in the last two years",
#                                    "Amount of deferrals due to low hemoglobin in the last two years",
#                                    "Time of day when donation was given as hours (e.g. 13:45 = 13.75)",
#                                    "Hb value at previous measurement (ICP-model)",
#                                    "Hb value at first donation of this donor (Non ICP-model)",
#                                    "Deferred based on low hemoglogin",
#                                   "Life time total count of donations",
#                                   "Sex"
#                    )
#                    )

descript <- descript %>% add_row(Variable="nb_donat", Pretty="Life time donations", Type="numeric (int)", Explanation="Life time total count of donations")
```

## Read and preprocess the data

### Read in enriched data. I DON'T USE THE ENRICHED DATA ANYMORE.
```{r}

#load(file="../data/enriched-2020-08-11-0.50-train-validate.processed.rdata")
#-rw-r--r-- 1 arvasmi arvasmi    487232 elo   20 13:45 enriched-2020-08-11-0.50-train-validate.processed.rdata

load_single <- function(filename) {
  names <- load(filename, verbose=FALSE)
  stopifnot(length(names) == 1)
  return(get(names))
}

#This data set has both train and validate together and both were enriched. The preprocessing was made in rf_caret_enriched_data_test.Rmd



train_orig <- load_single(file=sprintf("%s/preprocessed-2020-08-26-train.rdata", input_dir))
#train_orig <- sample_set(train_orig, 0.5)
#enriched <- load_single(file="../data/preprocessed-2020-08-26-train-enriched.rdata")
validate_orig <- load_single(file=sprintf("%s/preprocessed-2020-08-26-validate.rdata", input_dir))
test_orig <- load_single(file=sprintf("%s/preprocessed-2020-08-26-test.rdata", input_dir))
# -rw-r--r-- 1 arvasmi arvasmi  11195318 elo   26 11:08 preprocessed-2020-08-26-train-enriched.rdata
# -rw-r--r-- 1 arvasmi arvasmi  10810122 elo   26 10:28 preprocessed-2020-08-26-validate.rdata
#This data is split before enrichment and only train is enriched.
```

### Preprocess

```{r}
train_orig %>% ungroup() %>% count()
```

Full time series except those removed who only have one event or who's only deferral was the first event.


```{r}
validate_orig %>% ungroup() %>% count()
```


```{r}
train_orig %>% group_by(donor) %>% arrange(desc(dateonly)) %>% 
  dplyr::slice(1) %>% 
  ungroup() %>% group_by(Hb_deferral) %>% count()
```
Enriched to 50:50 balance.


```{r}
validate_orig %>% group_by(donor) %>% arrange(desc(dateonly)) %>% 
  dplyr::slice(1) %>% 
  ungroup() %>% group_by(Hb_deferral) %>% count()
```
Quite unbalanced as it should be.

Does the deferral percentage depend some how on donation carreer length
```{r , warning=FALSE}

temp <-validate_orig %>% ungroup() %>% 
  replace_na(list(nb_donat_progesa = 0,nb_donat_outside = 0)) %>% 
  group_by(donor) %>%
  arrange(donor,dateonly) %>% 
  mutate(nb_donat_run = row_number()) %>% 
  rowwise() %>% 
  mutate(
    nb_donat = sum( nb_donat_run, nb_donat_outside)) %>% 
      ungroup() %>% 
   select(Hb_deferral, nb_donat,gender) %>% 
  group_by(gender,nb_donat) %>% summarise(def = sum(Hb_deferral == 1),
                                          acc = sum(Hb_deferral == 0),
                                          pdef = def /(acc+def)
                                          ) 
ggplot(temp) + geom_line(aes(x=nb_donat,y=pdef,color=gender)) +xlim(0,50) + ylim(0,0.1)

```
```{r}
#temp<- temp %>% group_by(nb_donat) %>% summarise(def=sum(def),
 #                                         acc=sum(acc)) 
ggplot(temp) + geom_line(aes(x=nb_donat,y=def,color=gender,linetype='Deferral')) +geom_line(aes(x=nb_donat,y=acc,color=gender,linetype='Accepted')) + ylab("Count")
```
Yes, women have a lower deferral percentage in first event so removing the first event makes the overall deferral percentage in the data set bigger.

Take only last events and do come cleaning
```{r}
#Train
train <- train_orig %>% group_by(donor) %>% 
  arrange(desc(dateonly)) %>% 
  dplyr::slice(1) %>%   
  mutate(
    nb_donat = sum( nb_donat_progesa, nb_donat_outside ,na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  mutate(
    Hb_deferral=factor(Hb_deferral,labels = c("Accepted","Deferred"))
         ) %>% 
  filter( !nb_donat == '0') %>% #Some spurious mistake in the donation record
  filter(!(days_to_previous_fb < 62 & gender == "Men"))  %>% #Take out people that return faster than they should as we do not really know what they mean
  filter(!(days_to_previous_fb < 92 & gender == "Women"))

train <- train %>% ungroup() %>%   select(Hb_deferral,
                              days_to_previous_fb, 
                              age, 
                              previous_Hb_def,
                              #year, #CHECK! 
                              warm_season, 
                              consecutive_deferrals, 
                              recent_donations,
                              recent_deferrals, 
                              hour, 
                              previous_Hb, 
                              Hb_first, 
                              Hb_deferral,
                              nb_donat,
                              gender
)

#Validate


validate <-validate_orig %>% group_by(donor) %>%
  dplyr::filter(n()>1) %>%  #Take out the ones with only one event 
  arrange(desc(dateonly)) %>% 
  dplyr::slice(1) %>%   
  mutate(
    nb_donat = sum( nb_donat_progesa, nb_donat_outside ,na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  mutate(
    Hb_deferral=factor(Hb_deferral,labels = c("Accepted","Deferred"))
         ) %>% 
  filter( !nb_donat == '0') %>% #Some spurious mistake in the donation record
  filter(!(days_to_previous_fb < 62 & gender == "Men"))  %>% #Take out people that return faster than they should as we do not really know what they mean
  filter(!(days_to_previous_fb < 92 & gender == "Women"))


validate <- validate %>% ungroup() %>%   select(
  Hb_deferral,
  days_to_previous_fb, 
  age, 
  previous_Hb_def,
  #year, #CHECK! 
  warm_season, 
  consecutive_deferrals, 
  recent_donations,
  recent_deferrals, 
  hour, 
  previous_Hb, 
  Hb_first, 
  Hb_deferral,
  nb_donat,
  gender
)


#Test


test <-test_orig %>% group_by(donor) %>%
  dplyr::filter(n()>1) %>%  #Take out the ones with only one event 
  arrange(desc(dateonly)) %>% 
  dplyr::slice(1) %>%   
  mutate(
    nb_donat = sum( nb_donat_progesa, nb_donat_outside ,na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  mutate(
    Hb_deferral=factor(Hb_deferral,labels = c("Accepted","Deferred"))
         ) %>% 
  filter( !nb_donat == '0') %>% #Some spurious mistake in the donation record
  filter(!(days_to_previous_fb < 62 & gender == "Men"))  %>% #Take out people that return faster than they should as we do not really know what they mean
  filter(!(days_to_previous_fb < 92 & gender == "Women"))


test <- test %>% ungroup() %>%   select(
  Hb_deferral,
  days_to_previous_fb, 
  age, 
  previous_Hb_def,
  #year, #CHECK! 
  warm_season, 
  consecutive_deferrals, 
  recent_donations,
  recent_deferrals, 
  hour, 
  previous_Hb, 
  Hb_first, 
  Hb_deferral,
  nb_donat,
  gender
)


#Total donation count


```

Is there any NAs left?
```{r}
train %>% ungroup() %>%   summarise_all(funs(sum(is.na(.)))) %>% t()
```

```{r}
validate %>% ungroup() %>%   summarise_all(funs(sum(is.na(.)))) %>% t()
```


No.

### Summarise

How do the donation carreers seem on the last event?

```{r}
train %>% 
  group_by(Hb_deferral,gender) %>% 
  summarise(Ntot=n(),max=max(nb_donat),min=min(nb_donat),mean=mean(nb_donat),median=median(nb_donat)) %>% arrange(gender,Hb_deferral)
```

```{r}
validate %>% 
  group_by(Hb_deferral,gender) %>% 
  summarise(Ntot=n(),max=max(nb_donat),min=min(nb_donat),mean=mean(nb_donat),median=median(nb_donat)) %>% arrange(gender,Hb_deferral)
```

Hmmm, would this require more thought?

```{r}
summary(train)
```


```{r}
summary(validate)
```


```{r}
train %>% filter(gender=='Men') %>% summary()
```

```{r}
validate %>% filter(gender=='Men') %>% summary()
```



```{r}
train %>% filter(gender=='Women') %>% summary()
```

```{r}
validate %>% filter(gender=='Women') %>% summary()
```



## Decision tree

### Directly with rpart

Based on https://www.statmethods.net/advstats/cart.html 

```{r, eval=FALSE}
#trainxxx <- train[sample(nrow(train), 1000), ]

trtree <- rpart(Hb_deferral ~ ., 
      #control= rpart.control(minsplit=10, cp=0.01),   
      data= train , 
      method= "class"
      

)
trtree
```

### With caret

```{r Fit the decision tree, eval=TRUE}

file <- sprintf("%s/rrfFit__dtree_roc_hyper.rds", result_dir)

#https://rpubs.com/aryn999/DecisionTreeusingR -> but this has good example of class cut-off probability
 dtGrid <-  expand.grid(
#   cp = c(0.001887015, 0.003538153, 0.006545583, 0.038329992)
    cp = seq(0.0001, 0.04, length.out=10)
#   #split = c("gini", "information")  # caret allows only 'cp' as a tuning parameter
 )


trctrl <- trainControl(method = "repeatedcv", 
                       n = 4, 
                       repeats = 5,
                       classProbs = TRUE,
                       summaryFunction = twoClassSummary,
                       sampling = "up",
                       verboseIter = TRUE
)

if (compute) {
  caret_tree <- train(Hb_deferral ~ . ,
                      data = train, 
                      method = "rpart",
                      metric = "ROC",
                      parms  = list(split = "gini"), # This parameter goes to rpart::rpart
                      trControl = trctrl,
                      tuneGrid = dtGrid
                      #tuneLength = 10
  )
  saveRDS(caret_tree, file)
} else {
  caret_tree <- readRDS(file)  
}
caret_tree
trtree <- caret_tree$finalModel
```



```{r, eval=FALSE}
printcp(trtree)
```


```{r, eval=FALSE}
plotcp(trtree)
```

```{r, eval=TRUE}
#ptrtree<- prune(trtree, cp=   trtree$cptable[which.min(trtree$cptable[,"xerror"]),"CP"])
#trtree$cptable[which.min(trtree$cptable[,"xerror"]),"CP"]
```
There seems to be no reason to do any extra pruning.


```{r Visualize the decision tree, eval=TRUE}
rpart.plot(trtree,type = 4, extra = 101, tweak = 1)
```

```{r Save the above decision tree, eval=TRUE}
if (save_figs) {
  pdf(file=sprintf("%s/rf_no_splits_dtree_4-101.pdf", fig_dir), width = 14.4)
  rpart.plot(trtree,type = 4, extra = 101, tweak = 1)
  dev.off()
}
```

```{r Visualize another tree, eval=TRUE}
rpart.plot(trtree,type = 1, extra = 9, tweak = 1)
```

```{r Save the above visualization of the decision tree, eval=TRUE}
# This is shown in the supplement
if (save_figs) {
  pdf(file=sprintf("%s/rf_no_splits_dtree_1-9.pdf", fig_dir), width = 14.4)
  rpart.plot(trtree, type = 1, extra = 9, tweak = 1)
  dev.off()
}
```




```{r Decision tree confusion matrix validate 1, eval=TRUE}
pred.c.validate <- predict(caret_tree, validate)
#pred.c <- predict(caret_tree, validate, type = "class")
caret::confusionMatrix(data = pred.c.validate, reference=validate$Hb_deferral, positive = "Deferred")
```

```{r Decision tree confusion matrix validate 2, eval=TRUE}
caret::confusionMatrix(data = pred.c.validate, reference = validate$Hb_deferral, positive = "Deferred", mode = "prec_recall")
```
```{r Decision tree confusion matrix test, eval=TRUE}
pred.c.test <- predict(caret_tree, test)
caret::confusionMatrix(data = pred.c.test, reference = test$Hb_deferral, positive = "Deferred")
```



```{r Decision tree performance plots, eval=TRUE}
# This is shown in the supplement

#https://stackoverflow.com/questions/30818188/roc-curve-in-r-using-rpart-package
pred.p.validate <- predict(caret_tree, validate, type = "prob")
pred.p.test <- predict(caret_tree, test, type = "prob")

df <- cbind(pred.p.validate, obs=validate$Hb_deferral)
save(df, file=sprintf("%s/rrfFit__dtree_roc_validate_probs.rdata", result_dir))
df2 <- cbind(pred.p.test, obs=test$Hb_deferral)
save(df2, file=sprintf("%s/rrfFit__dtree_roc_test_probs.rdata", result_dir))

width = 180
#create_precision_recall_new <- function(labels, scores) {
df <- tibble(original_label=as.integer(validate$Hb_deferral=="Deferred"), score=pred.p.validate[,'Deferred'])
df2 <- tibble(original_label=as.integer(test$Hb_deferral=="Deferred"), score=pred.p.test[,'Deferred'])
#df <- data.frame(deferral=as.integer(validate$Hb_deferral) -1,scores=pred.p[,'Accepted'])
performances_validate <- create_performance_plots(
  df=df, 
  filename=sprintf("%s/rf_no_splits_dtree_validate_performances.pdf", fig_dir),
  width = 180
)
performances_test <- create_performance_plots(
  df=df2, 
  filename=sprintf("%s/rf_no_splits_dtree_test_performances.pdf", fig_dir),
  width = 180
)
performances_validate


```

```{r, eval=TRUE}
gc()
```


```{r Decision tree confusion plots, eval=TRUE}
cm_validate <- create_confusion_matrix_plot(as.integer(validate$Hb_deferral=="Deferred"), as.integer(pred.c.validate=="Deferred"))
cm_test <- create_confusion_matrix_plot(as.integer(test$Hb_deferral=="Deferred"), as.integer(pred.c.test=="Deferred"))
if (save_figs) {
  filename=sprintf("%s/rf_no_splits_dtree_validate_confusion.pdf", fig_dir)
  ggsave(filename=filename, cm_validate, width = 90, height = 80, units="mm", dpi=600, scale=1.0)
  filename=sprintf("%s/rf_no_splits_dtree_test_confusion.pdf", fig_dir)
  ggsave(filename=filename, cm_test, width = 90, height = 80, units="mm", dpi=600, scale=1.0)
}
#cm
```

```{r Decision tree variables importance, eval=TRUE}
rrfFit.varimp <- tibble("Variable"=names(trtree$variable.importance),"Importance"=as.numeric(trtree$variable.importance / sum(trtree$variable.importance)))
colnames(rrfFit.varimp) <- c("Variable","Importance")
rrfFit.varimp <- left_join(rrfFit.varimp, descript,by=c("Variable"="Variable")) %>% select(Variable,Pretty,Importance) %>% arrange(Importance)

rrfFit.varimp$Pretty[rrfFit.varimp$Variable == "previous_Hb_defTRUE"] <- "Previous donation deferred"
rrfFit.varimp$Pretty[rrfFit.varimp$Variable == "warm_seasonTRUE"] <- "Warm season"
rrfFit.varimp$Pretty[rrfFit.varimp$Variable == "genderWomen"] <- "Sex"

varimp.plot <- ggplot(rrfFit.varimp) + geom_col(aes(y=Importance ,x=reorder(Pretty, Importance)),alpha=0.7)+ coord_flip() + xlab("Variable")

filename=sprintf("%s/rf_no_splits_dtree_importance.pdf", fig_dir)
if (save_figs) {
  ggsave(filename=filename, varimp.plot, width = 180,  height = 80,units="mm", dpi=600, scale=1.0)
}
#varimp.plot
```


```{r Decition tree importance confusion plot, eval=TRUE}
# This is shown in the supplement
varimp.plot <- varimp.plot + theme(axis.text.x = element_text(angle = 90))
importance_confusion_validate <- cowplot::plot_grid(cm_validate, varimp.plot , labels = c('A', 'B'), label_size = 12, nrow=1, 
                                           scale=1.0, axis="tb", align="h")
importance_confusion_test <- cowplot::plot_grid(cm_test, varimp.plot , labels = c('A', 'B'), label_size = 12, nrow=1, 
                                      scale=1.0, axis="tb", align="h")
if (save_figs) {
  filename=sprintf("%s/rf_no_splits_dtree_validate_confusion_importance.pdf", fig_dir)
  cowplot::save_plot(filename, importance_confusion_validate, ncol = 2, base_asp = 1.0, base_width = width / 25.4 / 2, base_height = NULL)
  filename=sprintf("%s/rf_no_splits_dtree_test_confusion_importance.pdf", fig_dir)
  cowplot::save_plot(filename, importance_confusion_test, ncol = 2, base_asp = 1.0, base_width = width / 25.4 / 2, base_height = NULL)
}
importance_confusion_validate
#unloadNamespace("ModelMetrics")
```



Just doing a decision tree seems to already work well.




## Train random forest model

First do search for the correct hyperparameters with out saving much stuff.


```{r Random forest gridsearch big}
# ranges rf

#trainxxx <- train[sample(nrow(train), 1000), ]

file <- sprintf("%s/rrfFit_roc_hyper.rdata", result_dir)

#if (!file.exists(file)) {
if (compute) {
  #Initialise paralellisation
  cl <- makePSOCKcluster(4)
  registerDoParallel(cl)
  
  #Define search grid
  rrfGrid <-  expand.grid(
    mtry = c(3:8)
#    splitrule=c("gini", "extratrees","hellinger"),
    #min.node.size = seq(3,39,3) #what kind of range is needed?
#    min.node.size = 1
#    min.node.size = seq(1,60,5)
    #https://stats.stackexchange.com/questions/158583/what-does-node-size-refer-to-in-the-random-forest/
    #max.depth =c(1:10) # this is not available
  )
  
  #Define cross validation
  fitControl <- trainControl(## 4-fold CV
    method = "repeatedcv",
    number = 4, #how many is good?
    ## repeated ten times
    #repeats = 1, #how many is good?
    repeats = 5, #how many is good?
    classProbs = TRUE,
    summaryFunction = twoClassSummary
    #savePredictions = TRUE
  )
  
  # If I oversample too much, I get an error "There were missing values in resampled performance measures".
  # Probably because there were too many identical rows in the data, so the tree could not be build.
  # I solve this by only oversampling to halve of the most common class
  n <- sum(train$Hb_deferral == "Accepted") / 2  # oversample to the count of the most common class
  
  #Train
  rrfFit_roc_hyper <- train(Hb_deferral ~ ., data = train,
                            # The below three parameters go to randomForest::randomForest
                            strata=train$Hb_deferral,
                            sampsize=n,
                            replace=TRUE,
                            #method = "ranger", 
                            method = "rf", 
                            trControl = fitControl, 
                            verbose = FALSE, 
                            ## Now specify the exact models 
                            ## to evaluate:
                            tuneGrid = rrfGrid,
                            metric="ROC"
                            #importance=TRUE
                            #should we use Kappa or ROC?
                            #                importance = "permutation"
                            #https://stackoverflow.com/questions/18578861/variable-importance-using-the-caret-package-error-randomforest-algorithm
                            #should we use , ’impurity’, ’impurity_corrected’, ’permutation’ ?
  )
  
  save(rrfFit_roc_hyper, file=file)
  stopCluster(cl)
} else {
  load(file)
}
```



```{r}
rrfFit_roc_hyper
```


```{r Random forest grid search plot}
ghyper<- ggplot(rrfFit_roc_hyper)
if (save_figs) {
  ggsave(filename=sprintf("%s/rrfFit_roc_grid_search.pdf", fig_dir), ghyper, width = 180,  height = 120,units="mm", dpi=600, scale=1.0)
}
ghyper
```


With best hyperparameters do a run where all data is collected. (THERE'S NO REASON BECAUSE IT IS DONE ALREADY BY CARET)

```{r}
#rrfFit_roc <- rrfFit_roc_hyper
```


```{r Random forest grid search small, eval=TRUE}
# ranges rf

#trainxxx <- train[sample(nrow(train), 1000), ]
file <- sprintf("%s/rrfFit_roc.rdata", result_dir)

if (!file.exists(file)) {
  #Initialise paralellisation
  #cl <- makePSOCKcluster(4)
  #registerDoParallel(cl)

# rrfGrid <-  expand.grid(
#   mtry = c(4), #what kind of range is needed?
#   splitrule=c("hellinger"),
#   #min.node.size = c(32:40) #what kind of range is needed?
#   min.node.size = c(34) #what kind of range is needed?
#   #https://stats.stackexchange.com/questions/158583/what-does-node-size-refer-to-in-the-random-forest/
#   #max.depth =c(1:10) # this is not available
# )


# Not used.
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 4, #how many is good?
                           ## repeated ten times
                           repeats = 10, #how many is good?
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           savePredictions = TRUE
                           )


rrfFit_roc <- train(Hb_deferral ~ ., data =train, 
                 method = "rf", 
                 #trControl = fitControl,
                 trControl = caret::trainControl(method="none", classProbs = TRUE, summaryFunction = twoClassSummary), 
                 verbose = FALSE, 
                 ## Now specify the exact models 
                 ## to evaluate:
                 tuneGrid = rrfFit_roc_hyper$bestTune,
                metric="ROC",
                #should we use Kappa or ROC?
                importance = TRUE
                #importance = "permutation"
                #https://stackoverflow.com/questions/18578861/variable-importance-using-the-caret-package-error-randomforest-algorithm
                #should we use , ’impurity’, ’impurity_corrected’, ’permutation’ ?
                )

  save(rrfFit_roc, file=file)
  stopCluster(cl)
} else {
  load(file)
}

```


```{r}
rrfFit_roc
```


```{r, eval=FALSE}
ggplot(rrfFit_roc)
```





### Evaluate the model

```{r}
rrfFit_roc
```

ROC curve with the different folds
https://stackoverflow.com/questions/51904700/how-to-plot-roc-curves-for-every-cross-validations-using-caret
Do we need it.




```{r}
rrfFit_rocImp <- varImp(rrfFit_roc, scale = TRUE)
plot(rrfFit_rocImp)


```

```{r Random forest variable importance}
rrfFit.varimp <- as_tibble(cbind(rownames(rrfFit_rocImp$importance),rrfFit_rocImp$importance))
colnames(rrfFit.varimp) <- c("Variable","Importance")
rrfFit.varimp <- left_join(rrfFit.varimp, descript,by=c("Variable"="Variable")) %>% select(Variable,Pretty,Importance) %>% arrange(Importance)

rrfFit.varimp$Pretty[rrfFit.varimp$Variable == "previous_Hb_defTRUE"] <- "Previous donation deferred"
rrfFit.varimp$Pretty[rrfFit.varimp$Variable == "warm_seasonTRUE"] <- "Warm season"
rrfFit.varimp$Pretty[rrfFit.varimp$Variable == "genderWomen"] <- "Sex"

varimp.plot <- ggplot(rrfFit.varimp) + geom_col(aes(y=Importance ,x=reorder(Pretty, Importance)),alpha=0.7)+ coord_flip() + xlab("Variable")

if (save_figs) {
  filename=sprintf("%s/rrfFit_roc_importance.pdf", fig_dir)
  ggsave(filename=filename, varimp.plot, width = 180,  height = 80,units="mm", dpi=600, scale=1.0)
}
varimp.plot
```

See how the best model works with training data
```{r RF predict class train}
prediction.c.train <- predict(rrfFit_roc, newdata = train)
caret::confusionMatrix(data = prediction.c.train, reference = train$Hb_deferral, positive = "Deferred")
```


... and with validation data

```{r  RF predict class validate}
prediction.c.validate <- predict(rrfFit_roc, newdata = validate)
caret::confusionMatrix(data = prediction.c.validate, reference = validate$Hb_deferral, positive = "Deferred")
```


```{r}
caret::confusionMatrix(data = prediction.c.validate, reference = validate$Hb_deferral, positive = "Deferred", mode = "prec_recall")
```
```{r RF predict class test}
prediction.c.test <- predict(rrfFit_roc, newdata = test)
caret::confusionMatrix(data = prediction.c.test, reference = test$Hb_deferral, positive = "Deferred")
```


```{r RF predict probability validate}
prediction.probs.validate <- predict(rrfFit_roc, newdata = validate, type="prob")
df <- cbind(prediction.probs.validate, obs=validate$Hb_deferral)
save(df, file=sprintf("%s/rrfFit_roc_validate_probs.rdata", result_dir))

```


```{r RF predict probability test}
prediction.probs.test <- predict(rrfFit_roc, newdata = test, type="prob")
df <- cbind(prediction.probs.test, obs=test$Hb_deferral)
save(df, file=sprintf("%s/rrfFit_roc_test_probs.rdata", result_dir))
```

```{r}
#plot(validate.ROC)
```

```{r}
histogram(~prediction.probs.validate$Deferred|validate$Hb_deferral, xlab="Probability of Poor Segmentation")
```

And what does this mean?

### Report plots

```{r Random forest performance plots, warning=FALSE}
width = 180
#create_precision_recall_new <- function(labels, scores) {
df <- data.frame(original_label = as.integer(validate$Hb_deferral=="Deferred"), score = prediction.probs.validate$Deferred)
df2 <- data.frame(original_label = as.integer(test$Hb_deferral=="Deferred"), score = prediction.probs.test$Deferred)
if (save_figs) {
  filename <- sprintf("%s/rrfFit_roc_validate_performances.pdf", fig_dir)
  filename2 <- sprintf("%s/rrfFit_roc_test_performances.pdf", fig_dir)
} else {
  filaname <- NULL
  filaname2 <- NULL
}
performances_validate <- create_performance_plots(
  df=df, 
  filename=filename,
  width = 180
)
performances_test <- create_performance_plots(
  df=df2, 
  filename=filename2,
  width = 180
)
performances_validate
```

```{r}
gc()
```


```{r Random forest confusion matrix plot}
cm_validate <- create_confusion_matrix_plot(as.integer(validate$Hb_deferral=="Deferred"), 
                                            as.integer(prediction.c.validate=="Deferred"))
cm_test <- create_confusion_matrix_plot(as.integer(test$Hb_deferral=="Deferred"), 
                                            as.integer(prediction.c.test=="Deferred"))
if (save_figs) {
  filename=sprintf("%s/rrfFit_roc_validate_confusion.pdf", fig_dir)
  ggsave(filename=filename, cm_validate, width = 90,  height = 80, units="mm", dpi=600, scale=1.0)
  filename=sprintf("%s/rrfFit_roc_test_confusion.pdf", fig_dir)
  ggsave(filename=filename, cm_test, width = 90,  height = 80, units="mm", dpi=600, scale=1.0)
}
cm_validate
```

```{r Random forest combined confusion importance plot}

varimp.plot <- varimp.plot + theme(axis.text.x = element_text(angle = 90))
importance_confusion_validate <- cowplot::plot_grid(cm_validate, varimp.plot, labels = c('A', 'B'), label_size = 12, 
                                                    nrow=1, scale=1.0, axis="tb", align="h")
importance_confusion_test <- cowplot::plot_grid(cm_test, varimp.plot, labels = c('A', 'B'), label_size = 12, 
                                                    nrow=1, scale=1.0, axis="tb", align="h")
if (save_figs) {
  filename=sprintf("%s/rrfFit_roc_validate_confusion_importance.pdf", fig_dir)
  cowplot::save_plot(filename, importance_confusion_validate, ncol = 2, base_asp = 1.0, base_width = width / 25.4 / 2, base_height = NULL)
  filename=sprintf("%s/rrfFit_roc_test_confusion_importance.pdf", fig_dir)
  cowplot::save_plot(filename, importance_confusion_test, ncol = 2, base_asp = 1.0, base_width = width / 25.4 / 2, base_height = NULL)
}

```


### Effect of class assignment probabilty

```{r}
#cut prediction.probs with seq(0, 1, .1)

#df <- cbind(prediction.probs, obs=validate$Hb_deferral)
#save(df, file=sprintf("%s/rrfFit_roc_validate_probs.rdata", result_dir))
#x <- seq(0, 1, .1)
#out <- matrix(nrow=length(validate$Hb_deferral),ncol=length(x))
#colnames(out) <- paste0("p_",x)
#for (i in x ) {
#  out[,paste0("p_",i)] <- prediction.probs$Deferred <= i
#}
#save(out, file="../results/rrfFit_roc_assingments_with_probs.rdata")
#head(out)
```


### MLeval plots

https://stackoverflow.com/questions/31138751/roc-curve-from-training-data-in-caret

https://cran.r-project.org/web/packages/MLeval/vignettes/introduction.pdf


```{r calibration_curve}
library(MLeval)

res <- evalm(list1=df,showplots=FALSE,positive = "Deferred")

```


get calibration curve
```{r}
res$cc
```
get precision recall gain curve
```{r }
res$prg
```


### Distribution of data in confusion matrix cells

For training data

```{r}
add_confusion_class <- function(data, prediction) {
  conf <- bind_cols(data, "Confusion" = factor(paste0(data$Hb_deferral,"_",prediction))) %>% 
    mutate(Confusion= case_when(
      Confusion == "Accepted_Accepted" ~ "True negative",
      Confusion == "Deferred_Accepted" ~ "False negative",
      Confusion == "Deferred_Deferred" ~ "True positive",
      Confusion == "Accepted_Deferred" ~ "False positive"
    )) %>% 
    mutate(Confusion = fct_relevel(Confusion,
                                   "True positive",
                                   "False negative",
                                   "False positive",
                                   "True negative"
    ))
  return(conf)
}

new_names <- c(
      "Age"= "age", 
         "Days to previous \n full blood \n donation" = "days_to_previous_fb", 
         "Previous Hb \n deferral"= "previous_Hb_def", 
          "Warm season" = "warm_season", 
         "Consecutive deferrals" = "consecutive_deferrals", 
         "Recent \n donations" = "recent_donations", 
          "Recent \ndeferrals" = "recent_deferrals", 
        "Hour" = "hour", 
          "Previous Hb" = "previous_Hb", 
          "First Hb" = "Hb_first",
           "Consecutive \n deferrals" = "consecutive_deferrals",
        "Life time \n donations" = "nb_donat"
)
```

```{r, warning=FALSE}

#train.conf <- bind_cols(train,"Confusion"=factor(paste0("r_",train$Hb_deferral,":p_",prediction.tr)))
train.conf <- add_confusion_class(train, prediction.c.train)

pfemale <- train.conf %>% filter(gender == "Women") %>% 
  mutate(previous_Hb_def =as.numeric(previous_Hb_def), 
         warm_season = as.numeric(warm_season)) %>% 
  select(-Hb_deferral,-gender) %>%
  rename(!!new_names) %>% 
  gather("key","value",-Confusion) %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(color=Confusion)) +  theme(legend.position="bottom",
                                              legend.text= element_text(size=6)
                                                                          )

if (save_figs) {
  ggsave(filename=sprintf("%s/rrfFit_variables_train_female.pdf", fig_dir), pfemale, width = 180,  height = 180,
         units="mm", dpi=600, scale=1.0)
}
pfemale
```

Prediction seem to be mislead by Hb values that do not separate all cases well.


```{r}
pmale <- train.conf %>% filter(gender == "Men") %>% 
  mutate(previous_Hb_def =as.numeric(previous_Hb_def), 
         warm_season = as.numeric(warm_season)) %>% 
  select(-Hb_deferral,-gender) %>%
  rename(!!new_names) %>% 
  gather("key","value",-Confusion) %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(color=Confusion)) +  
  theme(legend.position="bottom", legend.text= element_text(size=6))


if (save_figs) {
  ggsave(filename=sprintf("%s/rrfFit_variables_train_male.pdf", fig_dir), pmale, width = 180,  height = 180,
         units="mm", dpi=600, scale=1.0)
}
pmale

```

For validation data

```{r}

validate.conf <- add_confusion_class(validate, prediction.c.validate)

#validate.conf <- bind_cols(validate,"Confusion"=factor(paste0("r_",validate$Hb_deferral,":p_",prediction.c.validate)))

pfemale <- validate.conf %>% filter(gender == "Women") %>% 
  mutate(previous_Hb_def =as.numeric(previous_Hb_def), 
         warm_season = as.numeric(warm_season)) %>% 
  select(-Hb_deferral,-gender) %>%
  rename(!!new_names) %>% 
  gather("key","value",-Confusion) %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(color=Confusion)) +
  theme(legend.position="bottom", legend.text= element_text(size=6))
if (save_figs) {
  ggsave(filename=sprintf("%s/rrfFit_variables_validate_female.pdf", fig_dir), pfemale, width = 180,  height = 180,
         units="mm", dpi=600, scale=1.0)
}

pfemale

```

```{r}

#validate.conf <- bind_cols(validate,"Confusion"=factor(paste0("r_",validate$Hb_deferral,":p_",prediction.c.validate)))

pmale <- validate.conf %>% filter(gender == "Men") %>% 
  mutate(previous_Hb_def =as.numeric(previous_Hb_def), 
         warm_season = as.numeric(warm_season)) %>% 
  select(-Hb_deferral,-gender) %>%
  rename(!!new_names) %>% 
  gather("key","value",-Confusion) %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(color=Confusion)) +
  theme(legend.position="bottom", legend.text= element_text(size=6))
if (save_figs) {
  ggsave(filename=sprintf("%s/rrfFit_variables_validate_male.pdf", fig_dir), pmale, width = 180,  height = 180,
         units="mm", dpi=600, scale=1.0)
}

pmale

```





### Test if 'ranger' case control weights change anything

```{r ranger_training}
#According to https://github.com/FRCBS/HSCT-relapse-model/blob/master/src/RF_relapse.R

c1w       <- 1-(sum(train$Hb_deferral=="Accepted") / nrow(train)) # set case weights
c2w       <- 1-(sum(train$Hb_deferral=="Deferred") / nrow(train))
cweights  <- sapply(train$Hb_deferral, function(x) ifelse(x=="Accepted", c1w, c2w))
#Model that predics classes
rf.model.pt  <- ranger(Hb_deferral ~., data=train, 
                    case.weights=cweights, 
                    probability=TRUE, 
                    num.trees=2500, 
                    importance='permutation',
                    min.node.size=34,
                    mtry=4,
                    splitrule="hellinger",
                    num.threads=4
                    )
#Model that predicts probabilites
rf.model.pf  <- ranger(Hb_deferral ~., 
                       data=train, 
                    case.weights=cweights, 
                    probability=FALSE, 
                    num.trees=2500, 
                    importance='permutation',
                    min.node.size=34,
                    mtry=4,
                    splitrule="hellinger",
                    num.threads=4
                    )
```


```{r}
rf.model.pt
```

```{r}
rf.model.pf
```


```{r}
# Isn't that reference parameter unnecessary in the predict call?
prediction <- predict(rf.model.pf, data = validate, reference = validate$Hb_deferral, num.threads=4)
caret::confusionMatrix(data = prediction$predictions, reference = validate$Hb_deferral, positive = "Deferred", mode="everything")

```



```{r}
prediction <- predict(rf.model.pt, data = validate, reference=validate$Hb_deferral, num.threads=4)

```

```{r create_performance_plots_rf_ranger}
# width = 180
# df <- data.frame(deferral=as.integer(validate$Hb_deferral) -1,scores=prediction$predictions[,'Deferred'])
# performances <- create_performance_plots(
#   df=df, 
#   filename="../results/rrfFit_roc__ranger_validate_performances.pdf",
#   width = 180
# )
# performances

```

```{r}
gc()
```


No, ranger case-control balancing does not help. We will skip that.
